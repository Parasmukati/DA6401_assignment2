# DA6401 Assignment 2: CNNs from Scratch & Transfer Learning with iNaturalist

This repository contains the solution to **Assignment 2** for the course **DA6401**, focused on training convolutional neural networks from scratch and fine-tuning pre-trained models using the [iNaturalist dataset](https://www.kaggle.com/competitions/inaturalist-2021-fgvc8/data).

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€ PartA.ipynb            # CNN model trained from scratch + W&B sweeps + visualizations
â”œâ”€â”€ PartB.ipynb            # Fine-tuning of a pre-trained model on iNaturalist
â”œâ”€â”€ data/                  # iNaturalist train/test folders (not included in repo)
â”œâ”€â”€ wandb/                 # W&B local logs
â”œâ”€â”€ README.md              
```

---

## ğŸš€ Assignment Overview

### ğŸ“Œ **Part A: CNN from Scratch**

#### âœ… Tasks Completed:
- Built a configurable CNN with 5 Conv-Activation-Pool blocks.
- Dense layer + output layer with 101 classes.
- Made kernel sizes, activation functions, and dense layer size configurable.
- Computed total parameters and computations for custom configurations.
- Split train/validation data with class balance.
- Implemented extensive hyperparameter tuning via `wandb` sweeps.
- Evaluated best model on test data.
- Visualized:
  - Model predictions on test set (10Ã—3 grid)
  - First-layer filters (optional)
  - Guided backpropagation (optional)

#### ğŸ” Key Hyperparameters Explored:
- Number of filters: 32, 64, 128
- Activation functions: ReLU, GELU, SiLU
- Filter scaling: constant, doubling, halving
- Dropout rates: 0.2, 0.3
- Batch normalization: enabled/disabled
- Data augmentation: enabled/disabled

#### ğŸ“Š W&B Visualizations:
- Accuracy vs Created plot
- Parallel coordinates plot
- Correlation summary

#### ğŸ“ Report Links:
- ğŸ§  Part A W&B Report: [W&B Project](https://api.wandb.ai/links/na21b051-indian-institute-of-technology-madras/f78cdtsl)
- ğŸ§¾ GitHub Code: [Part A Notebook](https://github.com/Parasmukati/DA6401_assignment2/blob/main/PartA.ipynb)

---

### ğŸ“Œ **Part B: Transfer Learning**

#### âœ… Tasks Completed:
- Loaded pre-trained model (e.g., ResNet50 from `torchvision.models`)
- Resized iNaturalist images to match ImageNet input dimensions.
- Replaced final classification layer with 101-class output layer.
- Explored fine-tuning strategies:
  - Freezing all layers except last
  - Freezing up to k layers
  - Full fine-tuning

#### ğŸ§ª Strategies Tried:
- Freeze all but last linear layer
- Freeze all conv layers and retrain classifier head
- Freeze first N blocks and fine-tune deeper layers

#### ğŸ“Œ Observations:
- Fine-tuning resulted in faster convergence and higher validation accuracy
- Smaller training time compared to training from scratch
- Transfer learning helped generalize better with limited data

#### ğŸ“ Report Links:
- ğŸ§  Part B W&B Report: [W&B Project](https://api.wandb.ai/links/na21b051-indian-institute-of-technology-madras/f78cdtsl)
- ğŸ§¾ GitHub Code: [Part B Notebook](https://github.com/Parasmukati/DA6401_assignment2/blob/main/PartB.ipynb)

---

## âš™ï¸ How to Run

### ğŸ”§ Requirements
- Python 3.8+
- PyTorch
- torchvision
- wandb

### ğŸƒ Setup

```bash
# Clone the repo
git clone https://github.com/<your-username>/da6401_assignment2.git
cd da6401_assignment2

# Install dependencies
pip install torch torchvision wandb
```

### ğŸ“Ÿ Running the notebooks
I used Google Colab with GPU enabled.

---
